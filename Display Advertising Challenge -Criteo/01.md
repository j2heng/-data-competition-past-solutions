## Display Advertising Challenge - Criteo (2015)
[Link to Kaggle ](https://www.kaggle.com/c/criteo-display-ad-challenge)
## 1.Data description

* The training set consists of a portion of Criteo's traffic over a period of 7 days. Each row corresponds to a display ad served by Criteo. Positive (clicked) and negatives (non-clicked) examples have both been subsampled at different rates in order to reduce the dataset size. The examples are chronologically ordered.
* Data fields: 
  * __Label - Target variable that indicates if an ad was clicked (1) or not (0).__
  * __I1-I13 - A total of 13 columns of integer features (mostly count features).__
  * __C1-C26 - A total of 26 columns of categorical features. The values of these features have been hashed onto 32 bits for anonymization purposes.__
  
  Though the exact semantic of the features is undisclosed to us, Olivier Chapell, the competition host, wrote that they fall in the following categories:
  * Publisher features, such as the domain of the url where the ad was displayed;
  * Advertiser features (advertiser id, type of products,…)
  * User features, for instance browser type;
  * Interaction of the user with the advertiser, such as the number of the times the user visited the advertiser website.

* Submissions are evaluated using the __Logarithmic Loss__

## 2.Training data statistics
Training data for this competition is very large - 1.1GB!  45,840,617 rows x 40 columns

Lable: The percentage of ads clicked (Label=1 is 25.6% (11,745,438 rows) 

Features
* Missing values: {I12, C22} have over 70% of missing values, {I1, I10, C19, C20, C25, C26} over 40%. 
* Range :  the range of Integer features are very different : max(I5)=2.315946e+07, max(I10)=1.100000e+01
* Cardinality:'unique' shows that {C3, C4, C12, C16, C21, C24} each feature has over 100.000 different values
>"freq": the most common value’s frequency

## 3.Methods
  
  Keywords: feature hashing, online learning, vowpal wabbit, FFM
  
### 0) [Baseline](https://www.kaggle.com/c/criteo-display-ad-challenge/discussion/9577#latest-53519) (shared by the competition host)  
   * All the features were encoded as sparse binary (standard "one hot" encoding);
   * To reduce the dimensionality, the features that took more than 10,000 different values on the training set were discarded; 
   * The learning algorithm was linear logistic regression optimized with l-bfgs;
   * The regularization coefficient was set to 1.
  
  Here are some top solutions :
### 1) tinrtgu shared [a very cool method](https://www.kaggle.com/c/criteo-display-ad-challenge/discussion/10322#latest-291615) which beat the benchmark with less then 200MB of memory using only standard Python libraries
#### step 0, define (bounded) logloss function _def logloss(p, y)_

For each row, get x and y, then:
#### step 1, apply hash trick on features x (for simplicity, treat both integer and categorical features as categorical) _def get_x_ 
Simply append the feature value and the numerical part of column name as a number in base16. Then convert it to decimal, modulo D (here D = 2 ** 20, number of weights use for learning)
 > - by using _DictReader_, inputs are formated to a csv dictionary as x = {'Lable': '1', 'I1': '357', 'I2': '', ...} 
 > - hash trick exemple,  _'C9': 'a73ee510'_  => append =>'a73ee5109' => to decimal => 44894671113 => modulo by D =>  938249 (hC9)
 > - each row retures x = [0, hI1, hI2, ..., hC26], where 0 is reserved for the index of the bias term 

 > the original code uses a "weak" hashing method which results in a bad collision rate. Triskelion suggested that __murmurhash__ may be a good choice. 

#### # step 2, get prediction p
 > - input: x = [0, hI1, hI2, ..., hC26] where each value h belongs to [0, D]
 > - for each i in x, _weight_ = _weight_ + _weight[i]_  (_weight_ initialiez as [0.] * D) 
 > - output: p = 1/exp(_weight_)  (bounded sigmoid ) 

#### # step 3, update weight with prediction p and true value y
 > - for each i in x, _weight[i] = weight[i] - (p - y) *lr_ (where lr is adaptive learning rate) 

Next row
