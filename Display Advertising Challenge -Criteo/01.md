## Display Advertising Challenge - Criteo (2015)
[Link to Kaggle ](https://www.kaggle.com/c/criteo-display-ad-challenge)
## 1.Data description

* The training set consists of a portion of Criteo's traffic over a period of 7 days. Each row corresponds to a display ad served by Criteo. Positive (clicked) and negatives (non-clicked) examples have both been subsampled at different rates in order to reduce the dataset size. The examples are chronologically ordered.
* Data fields: 
  * __Label - Target variable that indicates if an ad was clicked (1) or not (0).__
  * __I1-I13 - A total of 13 columns of integer features (mostly count features).__
  * __C1-C26 - A total of 26 columns of categorical features. The values of these features have been hashed onto 32 bits for anonymization purposes.__
  
  Though the exact semantic of the features is undisclosed to us, Olivier Chapell, the competition host, wrote that they fall in the following categories:
  * Publisher features, such as the domain of the url where the ad was displayed;
  * Advertiser features (advertiser id, type of products,…)
  * User features, for instance browser type;
  * Interaction of the user with the advertiser, such as the number of the times the user visited the advertiser website.

* Submissions are evaluated using the __Logarithmic Loss__

## 2.Training data statistics
Training data for this competition is very large - 1.1GB!  45,840,617 rows x 40 columns

Lable: The percentage of ads clicked (Label=1 is 25.6% (11,745,438 rows) 

Features
* Missing values: {I12, C22} have over 70% of missing values, {I1, I10, C19, C20, C25, C26} over 40%. 
* Range :  the range of Integer features are very different : max(I5)=2.315946e+07, max(I10)=1.100000e+01
* Cardinality:'unique' shows that {C3, C4, C12, C16, C21, C24} each feature has over 100.000 different values
>"freq": the most common value’s frequency

## 3.Methods
  
  Keywords: feature hashing, online learning, vowpal wabbit, FFM
  
### 0) [baseline](https://www.kaggle.com/c/criteo-display-ad-challenge/discussion/9577#latest-53519) (shared by the competition host)  
   * All the features were encoded as sparse binary (standard "one hot" encoding);
   * To reduce the dimensionality, the features that took more than 10,000 different values on the training set were discarded; 
   * The learning algorithm was linear logistic regression optimized with l-bfgs;
   * The regularization coefficient was set to 1.
  
  Here are some top solutions :
### 1) tinrtgu shared [a very cool method](https://www.kaggle.com/c/criteo-display-ad-challenge/discussion/10322#latest-291615) which beat the benchmark with less then 200MB of memory using only standard Python libraries
__In short, one-pass SGD, feature hashing__. For each row, get x and y, then :

#### step 1, apply hash trick on features x (for simplicity, treat both integer and categorical features as categorical) _def get_x_ 
Simply append the feature value and the numerical part of column name as a number in base16. Then convert it to decimal, modulo D (here D = 2 ** 20, number of weights use for learning)
 > - by using _DictReader_, inputs are formated to a csv dictionary as x = {'Lable': '1', 'I1': '357', 'I2': '', ...} 
 > - hash trick exemple,  _'C9': 'a73ee510'_  => append =>'a73ee5109' => to decimal => 44894671113 => modulo by D =>  938249, __this integer 938249 does not mean that a feature's value is 938249; it actually means that the value of 938249nd feature is non-zero.__
 > - each row retures x = [0, hashed_I1, hashed_I2, ..., hashed_C26], where 0 is reserved for the index of the bias term 

 > the original code uses a "weak" hashing method which results in a bad collision rate. Triskelion suggested that __murmurhash__ may be a good choice. 

#### # step 2, get prediction p
 > - input: x = [0, hashed_I1,  hashed_I2, ..., hashed_C26] where each hashed value belongs to [0, D]
 > - for each i in x, _weight_ = _weight_ + _weight[i]_  (_weight_ initialiez as [0.] * D) 
 > - output: p = 1/exp(_weight_)  (bounded sigmoid ) 

#### # step 3, update weight with prediction p and true value y 
 > - for each i in x, _weight[i] = weight[i] - (p - y) *lr_ (where lr is adaptive learning rate, depending on the number of times we encounter a feature) 

Next row


### 2) winning team (3 Idiots’) solution [code here](https://github.com/ycjuan/kaggle-2014-criteo)
__In short, GBDT and Field-aware Factorization Machines (FFM)__
#### # step 1, preprocessing-A, generate features for GBDT
- All numerical data are included. (13 features); 
- Categorical features (after one-hot encoding) appear more than 4 million times are also included. (26 features)

#### # step 2, GBDT, generate GBDT features
Train a GBDT model with existing features from step 1, then use the trees learned by the GBDT model to generate new features. Here, 30 trees with a depth of 7 were used. Thus 30 new features are generated for each impression.

Example: Assuming that we have already trained GBDT with 3 trees with depth 2.We feed an impression x into these trees. The first tree thinks x belong to node 4, the second node 7, and the third node 6. Then we generate the feature ”1:4 2:7 3:6” for this impression.

#### # step 3, preprocessing-B, generate features for FFM
Each impression has 13 (numerical) + 26 (categorical) + 30(GBDT) = 69 features.
- Numerical features (I1-I13) greater than 2 are transformed to reduce the number of features generated: v <- floor(log(v)^2) 
- Categorical features (C1-C26) appear less than 10 times are transformed into a special value.
- GBDT features are directly included.
These three groups of features are hashed into 1M-dimension by hashing trick.

Example:  text => (hash function)=> hashed value => (mod 10^6) => feature

text | hashed value | feature
------|----------|------
I1:3 | 739920192382357839297 | 839297
C1-68fd1e64 | 839193251324345167129 | 167129
GBDT1:173 |923490878437598392813 | 392813

#### # step 4, Field-aware Factorization Machines (FFM) [pdf here](https://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf)
In order to use the FFM method, all features must be converted to the "field_id:feat_id:value" format, “field_id” represents the number of the field to which the feature belongs, “feat_id” is the feature number, and “value” is the value of the feature.

- field_id: I1<->0, ... ,I13<->12, C1<->13, ... , C26<->38, GBDT1<->39, ... , GBDT30<->68
- feat_id: hashstr(feaure) where hashstr is ```int(hashlib.md5(str.encode('utf8')).hexdigest(), 16)%(10^6-1)+1```
- value : 1, it means that the value of feat_id'th feature is non-zero

Example "field_id:feat_id:value": 
- I1:3 becomes "0:580972:1" where 580972 = hashstr(839297, 10^6),
- C1-68fd1e64 becomes "13:389252:1" where 389252 = hashstr(167129, 10^6), 
- GBDT1:173 becomes "39:607955:1" where 607955 = hashstr(392813, 10^6)


### 3) online machine learning with [Vowpal Wabbit](https://github.com/VowpalWabbit/vowpal_wabbit)
> Vowpal Wabbit (also known as "VW") is an open-source fast out-of-core machine learning system library and program developed originally at Yahoo! Research, and currently at Microsoft Research
> Its scalability is aided by several factors:
> - Out-of-core online learning: no need to load all data into memory
> - The hashing trick: feature identities are converted to a weight index via a hash (uses 32-bit MurmurHash3)
> - Exploiting multi-core CPUs: parsing of input and learning are done in separate threads.
> - Compiled C++ code

[3rd place (Guocong Song) solution](https://github.com/songgc/display-advertising-challenge)

[fastML - Vowpal Wabbit eats big data from the Criteo competition for breakfast](http://fastml.com/vowpal-wabbit-eats-big-data-from-the-criteo-competition-for-breakfast/)
