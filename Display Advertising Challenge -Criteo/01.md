## Display Advertising Challenge - Criteo (2015)
[Link to Kaggle ](https://www.kaggle.com/c/criteo-display-ad-challenge)
## 1.Data description

* The training set consists of a portion of Criteo's traffic over a period of 7 days. Each row corresponds to a display ad served by Criteo. Positive (clicked) and negatives (non-clicked) examples have both been subsampled at different rates in order to reduce the dataset size. The examples are chronologically ordered.
* Data fields: 
  * __Label - Target variable that indicates if an ad was clicked (1) or not (0).__
  * __I1-I13 - A total of 13 columns of integer features (mostly count features).__
  * __C1-C26 - A total of 26 columns of categorical features. The values of these features have been hashed onto 32 bits for anonymization purposes.__

  > what does it mean "hashed onto 32 bits": it can have at most 2**32 cardinality. By default, use base16, one digit in base16 <-> 4 bits in base2 (e.g. a: 1010, f:1111). Thus after hashing, the value is represented in 8 digits (=32/4) (e.g. 68fd1e64)
  
  Though the exact semantic of the features is undisclosed to us, Olivier Chapell, the competition host, wrote that they fall in the following categories:
  * Publisher features, such as the domain of the url where the ad was displayed;
  * Advertiser features (advertiser id, type of products,…)
  * User features, for instance browser type;
  * Interaction of the user with the advertiser, such as the number of the times the user visited the advertiser website.

* Submissions are evaluated using the __Logarithmic Loss__

## 2.Training data statistics
Training data for this competition is very large - 1.1GB!  45,840,617 rows x 40 columns

The percentage of ads clicked (Label=1): 25.6% (11,745,438 rows) 

stat | I1 |	I2	| I3	| I4	| I5	| I6	| I7	| I8	| I9	| I10	| I11	| I12	| I13
------|------|------|------|------|------|------|------|------|------|------|------|------|------|
count | 2.504706e+07|	4.584062e+07|	3.600117e+07|	3.590325e+07|	4.465750e+07|	3.558829e+07|	4.385775e+07|	4.581784e+07|	4.385775e+07|	2.504706e+07|	4.385775e+07|	1.076896e+07|	3.590325e+07	
mean	|	3.502413e+00|	1.058484e+02|	2.691304e+01|	7.322680e+00|	1.853899e+04|	1.160619e+02|	1.633313e+01|	1.251704e+01|	1.061098e+02|	6.175295e-01|	2.732834e+00|	9.910356e-01|	8.217461e+00	
max	|	5.775000e+03|	2.576750e+05|	6.553500e+04|	9.690000e+02|	2.315946e+07|	4.310370e+05|	5.631100e+04|	6.047000e+03|	2.901900e+04|	1.100000e+01|	2.310000e+02|	4.008000e+03|	7.393000e+03


stat | C1|	C2|	C3|	C4|	C5|	C6|	C7|	C8|	C9|	C10|	C11|	C12|	C13|	C14|	C15|	C16|	C17|	C18|	C19|	C20|	C21|	C22|	C23|	C24|	C25|	C26
------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|
count | 45840617|	45840617|	44281144|	44281144|	45840617|	40299992|	45840617|	45840617|	45840617|	45840617|	45840617|	44281144|	45840617|	45840617|	45840617|	44281144|	45840617|	45840617|	25667759|	25667759|	44281144|	10885544|	45840617|	44281144|	25667759|	25667759
unique	|	1460|	583|	10131226|	2202607|	305|	23|	12517|	633|	3|	93145|	5683|	8351592|	3194|	27|	14992|	5461305|	10|	5652|	2172|	3|	7046546|	17|	15|	286180|	104|	142571
freq	|	22950860|	5245774|	1141289|	1636423|	30772137|	18166950|	955418|	27232196|	41200152|	10157763|	1458428|	1141289|	1458428|	16029055|	684365|	1141316|	21153559|	1450322|	15800355|	8702869|	1141289|	6253065|	20170568|	2400439|	6557407	| 1888619

Observations
* Missing values: {I12, C22} have over 70% of missing values, {I1, I10, C19, C20, C25, C26} over 40%. 
* Range :  the range of Integer features are very different : max(I5)=2.315946e+07, max(I10)=1.100000e+01
* Cardinality:'unique' shows that {C3, C4, C12, C16, C21, C24} each feature has over 100.000 different values
>"freq": the most common value’s frequency

## 3.Methods
  
  Keywords: feature hashing, online learning, vowpal wabbit, FFM
  
### 0) [Baseline](https://www.kaggle.com/c/criteo-display-ad-challenge/discussion/9577#latest-53519) (shared by the competition host)  
   * All the features were encoded as sparse binary (standard "one hot" encoding);
   * To reduce the dimensionality, the features that took more than 10,000 different values on the training set were discarded; 
   * The learning algorithm was linear logistic regression optimized with l-bfgs;
   * The regularization coefficient was set to 1.
  
  Here are some top solutions :
### 1) tinrtgu shared [a very cool method](https://www.kaggle.com/c/criteo-display-ad-challenge/discussion/10322#latest-291615) which beat the benchmark with less then 200MB of memory using only standard Python libraries
#### step 1, get the hashed features (for simplicity, treat both integer and categorical features as categorical) 
Simply append the feature value and the numerical part of column name as a number in base16. Then convert it to decimal, modulo D (here D = 2 ** 20, number of weights use for learning)
> exemple: ('C9', 'a73ee510') becomes 'a73ee5109' => to decimal => 44894671113 => modulo by D =>  938249

```
def get_x(csv_row, D):
    x = [0]  # 0 is the index of the bias term
    for key, value in csv_row.items():
        index = int(value + key[1:], 16) % D  # weakest hash ever ;)
        x.append(index)
    return x  # x contains indices of features that have a value of 1
```    
> int(value, base) converts a number in given base to decimal.
```
for t, row in enumerate(DictReader(open(train))):
  x = get_x(row, D)
```
> the original code uses a "weak" hashing method which results in a bad collision rate. Triskelion suggested that __murmurhash__ may be a good choice. 
  
#### # step 2, get prediction
```
# initialize our model
w = [0.] * D  # weights
n = [0.] * D  # number of times we've encountered a feature

def get_p(x, w):
    wTx = 0.
    for i in x:  # do wTx
        wTx += w[i] * 1.  # w[i] * x[i], but if i in x we got x[i] = 1.
    return 1. / (1. + exp(-max(min(wTx, 20.), -20.)))  # bounded sigmoid
```

```
p = get_p(x, w)
```
